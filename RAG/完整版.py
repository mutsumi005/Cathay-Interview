import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_classic.chains import RetrievalQA

# ======================
# 1. Embedding (ä¿æŒè¼•é‡)
# ======================
print("ğŸ“¦ åˆå§‹åŒ–å‘é‡æ¨¡å‹...")
embedding = HuggingFaceEmbeddings(
    model_name="BAAI/bge-small-zh-v1.5",
    model_kwargs={"device": "cuda"},
    encode_kwargs={"normalize_embeddings": True}
)

# ======================
# 2. è¼‰å…¥ FAISS ç´¢å¼•
# ======================
print("âœ… è¼‰å…¥å·²å­˜åœ¨ç´¢å¼•...")
vectorstore = FAISS.load_local(
    "insurance_faiss",
    embedding,
    allow_dangerous_deserialization=True
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

# ======================
# 3. è¼‰å…¥ Qwen2-1.5B (é‡å° 6GB VRAM å„ªåŒ–)
# ======================
model_id = "Qwen/Qwen2-1.5B-Instruct" 

print(f"ğŸ”¥ æ­£åœ¨ä»¥åŸç”Ÿ FP16 æ¨¡å¼è¼‰å…¥ {model_id}...")
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16, 
    device_map="auto",
    trust_remote_code=True
)

# ã€é—œéµé» 1ã€‘åœ¨ pipeline ä¸­åŠ å…¥åœæ­¢èˆ‡æ‡²ç½°åƒæ•¸
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    temperature=0.1,    # é™ä½éš¨æ©Ÿæ€§ï¼Œè®“å›ç­”æ›´æº–ç¢º
    top_p=0.9,
    repetition_penalty=1.1, # æŠ‘åˆ¶æ¨¡å‹ã€Œé¬¼æ‰“ç‰†ã€é‡è¤‡èªªè©±
    do_sample=True,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.eos_token_id
)

llm = HuggingFacePipeline(pipeline=pipe)

# ======================
# 4. Prompt èˆ‡ Chain
# ======================
# ã€é—œéµé» 2ã€‘æ”¹ç”¨ Qwen å°ˆç”¨çš„ ChatML æ ¼å¼æ¨™ç±¤
template = """<|im_start|>system
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ä¿éšªé¡§å•ï¼Œè«‹æ ¹æ“šæä¾›çš„æ¢æ¬¾å…§å®¹ï¼Œç²¾ç¢ºä¸”ç°¡æ½”åœ°å›ç­”å•é¡Œã€‚å¦‚æœæ¢æ¬¾ä¸­æ²’æåˆ°ï¼Œè«‹å›ç­”ä¸çŸ¥é“ã€‚å›ç­”å®Œç•¢è«‹åœæ­¢ã€‚
<|im_end|>
<|im_start|>user
æ¢æ¬¾å…§å®¹ï¼š
{context}

å•é¡Œï¼š{question}
<|im_end|>
<|im_start|>assistant
"""

prompt = PromptTemplate(template=template, input_variables=["context", "question"])

qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": prompt},
    return_source_documents=True
)

# ======================
# 5. Gradio ä»‹é¢å‡½å¼
# ======================
def ask_insurance(question):
    if not question.strip(): return "è«‹è¼¸å…¥å•é¡Œ", ""
    try:
        result = qa.invoke({"query": question})
        raw_answer = result["result"]
        
        # ã€é—œéµé» 3ã€‘æ¸…ç† LLM è¼¸å‡ºçš„æ¨™ç±¤å…§å®¹
        # ç”±æ–¼ LLM å¯èƒ½æœƒå›å‚³åŒ…å« Prompt çš„å…¨æ–‡ï¼Œæˆ‘å€‘åªå– assistant ä¹‹å¾Œçš„å…§å®¹
        if "<|im_start|>assistant" in raw_answer:
            answer = raw_answer.split("<|im_start|>assistant")[-1].strip()
        else:
            answer = raw_answer.strip()
            
        # ç§»é™¤å¯èƒ½æ®˜ç•™çš„çµæŸç¬¦è™Ÿ
        answer = answer.replace("<|im_end|>", "").strip()
            
        sources = "\n\n".join([f"ğŸ“„ æ¢æ¬¾æ‘˜éŒ„ï¼š\n{doc.page_content[:300]}" for doc in result["source_documents"]])
        return answer, sources
    except Exception as e:
        return f"ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}", ""

# ======================
# 6. å•Ÿå‹•ä»‹é¢
# ======================
with gr.Blocks(title="ä¿éšª RAG ç³»çµ±") as demo:
    gr.Markdown("# ğŸ§³ è¼•é‡ç‰ˆä¿éšªæ¢æ¬¾å•ç­”ç³»çµ±\né‡å° 6GB é¡¯å­˜å„ªåŒ– (Qwen2-1.5B) - å·²ä¿®å¾©é‡è¤‡å•é¡Œ")
    with gr.Row():
        with gr.Column(scale=1):
            q = gr.Textbox(label="è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ", placeholder="ä¾‹å¦‚ï¼šè¡Œæéºå¤±å¾Œè©²å¦‚ä½•ç”³è«‹ç†è³ ï¼Ÿ")
            btn = gr.Button("æŸ¥è©¢æ¢æ¬¾", variant="primary")
        with gr.Column(scale=2):
            ans = gr.Textbox(label="ğŸ“Œ é¡§å•å›ç­”", lines=10)
            src = gr.Textbox(label="ğŸ“„ ä¾æ“šæ¢æ¬¾", lines=5)
    
    btn.click(ask_insurance, inputs=q, outputs=[ans, src])

# å•Ÿå‹•æ™‚è‡ªå‹•é–‹å•Ÿç€è¦½å™¨
if __name__ == "__main__":

    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)
