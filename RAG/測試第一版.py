import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig
from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_classic.chains import RetrievalQA

# ======================
# 1. Embedding (ä¿æŒè¼•é‡)
# ======================
print("ğŸ“¦ åˆå§‹åŒ–å‘é‡æ¨¡å‹...")
embedding = HuggingFaceEmbeddings(
    model_name="BAAI/bge-small-zh-v1.5",
    model_kwargs={"device": "cuda"},
    encode_kwargs={"normalize_embeddings": True}
)

# ======================
# 2. è¼‰å…¥ FAISS ç´¢å¼•
# ======================
print("âœ… è¼‰å…¥å·²å­˜åœ¨ç´¢å¼•...")
vectorstore = FAISS.load_local(
    "insurance_faiss",
    embedding,
    allow_dangerous_deserialization=True
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 2}) # å†æ¬¡æ¸›å°‘ K å€¼ä»¥ç¯€çœé¡¯å­˜

# ======================
# 3. è¼‰å…¥ Qwen2-1.5B (ä¿è­‰èƒ½å‹•çš„ç‰ˆæœ¬)
# ======================
# æ”¹ç”¨ 1.5B æ¨¡å‹ï¼Œé¡¯å­˜ä½”ç”¨ç´„ 1.5GB ~ 2GB
# ======================

# ======================
# ======================
# 3. è¼‰å…¥ Qwen2-1.5B (åŸç”Ÿ FP16 ç‰ˆ)
# ======================
model_id = "Qwen/Qwen2-1.5B-Instruct" 

print(f"ğŸ”¥ æ­£åœ¨ä»¥åŸç”Ÿ FP16 æ¨¡å¼è¼‰å…¥ {model_id}...")

tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)

# æ ¸å¿ƒä¿®æ”¹ï¼šä¸ä½¿ç”¨ quantization_configï¼Œç›´æ¥ç”¨ torch_dtype
# 1.5B æ¨¡å‹åœ¨ float16 æ¨¡å¼ä¸‹åƒ…ä½”ç´„ 3.2GB é¡¯å­˜ï¼Œä½ çš„ 6GB é¡¯å¡ç¶½ç¶½æœ‰é¤˜ï¼
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16, 
    device_map="auto",          # åŸç”Ÿ FP16 æ¨¡å‹æ”¯æ´è‡ªå‹•åˆ†é…
    trust_remote_code=True
)

# pipeline ä¿æŒä¸è®Š
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    temperature=0.1,
    do_sample=True,
)

llm = HuggingFacePipeline(pipeline=pipe)
# ======================
# 4. Prompt èˆ‡ Chain
# ======================
template = """ä½ æ˜¯ä¸€ä½ä¿éšªå°ˆæ¥­é¡§å•ï¼Œè«‹æ ¹æ“šæä¾›çš„ã€Œæµ·å¤–æ—…è¡Œä¸ä¾¿éšªæ¢æ¬¾ã€ç²¾ç¢ºå›ç­”å•é¡Œã€‚

æ¢æ¬¾å…§å®¹ï¼š
{context}

å•é¡Œï¼š{question}
ç­”æ¡ˆï¼š"""

prompt = PromptTemplate(template=template, input_variables=["context", "question"])

# æ³¨æ„ï¼šç¢ºä¿ä½ çš„ langchain æ˜¯æ–°ç‰ˆï¼Œå¦å‰‡è«‹ç¶­æŒåŸæœ¬çš„åŒ¯å…¥æ–¹å¼
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": prompt},
    return_source_documents=True
)

# ======================
# 5. Gradio ä»‹é¢å‡½å¼
# ======================
def ask_insurance(question):
    if not question.strip(): return "è«‹è¼¸å…¥å•é¡Œ", ""
    try:
        # ä½¿ç”¨ invoke é¿å…èˆŠç‰ˆè­¦å‘Š
        result = qa.invoke({"query": question})
        answer = result["result"]
        # è™•ç†æ¨¡å‹å¯èƒ½çš„å†—é•·è¼¸å‡ºï¼ŒåªæŠ“ç­”æ¡ˆéƒ¨åˆ†
        if "ç­”æ¡ˆï¼š" in answer:
            answer = answer.split("ç­”æ¡ˆï¼š")[-1].strip()
            
        sources = "\n\n".join([f"ğŸ“„ æ¢æ¬¾æ‘˜éŒ„ï¼š\n{doc.page_content[:300]}" for doc in result["source_documents"]])
        return answer, sources
    except Exception as e:
        return f"ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}", ""

# ======================
# 6. å•Ÿå‹•ä»‹é¢
# ======================
with gr.Blocks(title="ä¿éšª RAG ç³»çµ±") as demo:
    gr.Markdown("# ğŸ§³ è¼•é‡ç‰ˆä¿éšªæ¢æ¬¾å•ç­”ç³»çµ±\né‡å° 6GB é¡¯å­˜å„ªåŒ– (Qwen2-1.5B)")
    with gr.Row():
        with gr.Column(scale=1):
            q = gr.Textbox(label="è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ", placeholder="ä¾‹å¦‚ï¼šç­æ©Ÿå»¶èª¤æ€éº¼è³ ï¼Ÿ")
            btn = gr.Button("æŸ¥è©¢æ¢æ¬¾", variant="primary")
        with gr.Column(scale=2):
            ans = gr.Textbox(label="ğŸ“Œ é¡§å•å›ç­”", lines=10)
            src = gr.Textbox(label="ğŸ“„ ä¾æ“šæ¢æ¬¾", lines=5)
    
    btn.click(ask_insurance, inputs=q, outputs=[ans, src])

# å•Ÿå‹•æ™‚è‡ªå‹•é–‹å•Ÿç€è¦½å™¨
demo.launch(server_name="0.0.0.0", server_port=7860, share=False)
